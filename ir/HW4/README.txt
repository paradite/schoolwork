This is the README file for A0093910H's submission

== Personal Information ==
Matric No: A0093910H
Email: a0093910@u.nus.edu

== General Notes about this assignment ==
Overview:
The patent retrieval system first indexes terms from the title and abstract of a fixed set of patents.
When given a query, it performs first round of search using VSM model with lnc.ltc ranking scheme. Terms
within the top ranked patent documents are then retrieved for query expansion, with stopwords removed.
The second round of search returns all relevant documents, ranked by relevance score, as the final result.

Optional components:
Query expansion:
	In the retrieval system, we have optimized two key parameters to make query expansion effective, 
	these are empirical result that can be supported by theoratical understanding:

	TOP_LIST_LENGTH = 6
	We choose top 6 documents from the first round of search to form the basis of the expanded query.
	Higher number of documents will add additional noise to the relevant terms and result in lower precision.

	NO_OF_EXPANDED_QUERIES = 20
	We choose top 20 terms from these documents based on term frequency to be the expanded query.
	We use term frequency instead of tf-idf to rank the terms to avoid giving weights to rare words that
	only appear in a small set of documents and do not contribute positively to recall.
	20 queries are enough to cover most of the key terms that a relevant document should contain.
	When executing the query, we use the actual number of occurences for each of the 20 terms, for example,
	if the term "water" has appeared 20 times in the top 6 documents, we will have 20 "water" in the query,
	this allows the calculation of tf on expanded query during second round of search.

Stopwords/external resource:
	In the retrieval system, we perform exclusion of stopwords in the expaneded query.
	We used tf in ranking the terms for expanded query to avoid high weights on rare words and 
	make common words rank higher. However, this would give high weights to stopwords. Hence, we have used
	a set of stopwords as a blacklist.
	The set of stopwords includes the nltk stopwords for English as well as a cutom-make set of commonly
	appearing non-descriptive words in patents, such as "method" and "system".
	The use of stopwords allows us to have the most relevant terms in the expanded query that in turn gives
	the most relevant documents in second round of searching.

Indexing: 
A corpus is first generated by reading files in patsnap-corpus folder. The content of each patent xml file
is parsed and title and abstract fields are tokenized into terms.
Stemming and case-folding is then performed on each token to produce terms that will be stored
in dictionary together with the associated document frequency. 
Postings list is generated with raw term frequency of the term in each patent document.

In addition, another document named "docinfo.txt" is generated.
It contains the document lengths of each patent document, for length normalization during searching, 
as well as all the terms (including duplicates to reflect term frequency) in the title and abstract
of the patent document to allow for query expansion during search phase.

Searching:
Dictionary and information on length and terms in each patent document are read into the memory from 
the respective files.
For each query xml, the title and description are parsed, with terms extracted to use VSM model for 
ranking each document in the collection with lnc.ltc ranking scheme.
After we obtain the list of documents ranked by relevance, we select the top n documents as the basis 
for query expansion. 
From these n documents, we collect all the terms in title and abstract, remove the stopwords and rank them using tf.
Stop words inclue common English stop words and non-descriptive words that are specific to patents 
(such as "method" or "system". This makes sure that we are getting the most relevant terms. 
The use of tf instead of tf-idf in ranking ensures that we do not prioritize the rare words that only 
appear a few times in the entire collection.
to get the most relevant queries and use the top m terms with their actual number of repeated occurences
as the expanded query for the second round of searching.
In the second round of searching, we use return all retrieved documents ranked by their relevance to the output
without setting threshold for the relevance score. This allows for higher recall.

== Files included with this submission ==

ESSAY.txt		 - answers to essay questions
README.txt		 - readme file
utility.py 	     - utility module for indexing and searching operation
index.py 	     - indexes xml patent files to dictionary, posting and produce docinfo.txt
search.py        - searches dictionary and posting given an xml query file
docinfo.txt      - txt file containing document length and terms for each document
dictionary.txt   - txt file containing the dictionary with document frequency
postings.txt     - txt file containing postings list
ordereddict.py	 - OrderedDict's implemenation by Raymond Hettinger for python version < 2.7

== Statement of individual work ==

Please initial one of the following statements.

[x] I, A0093910H, certify that we have followed the CS 3245 Information
Retrieval class guidelines for homework assignments.  In particular, I
expressly vow that I have followed the Facebook rule in discussing
with others in doing the assignment and did not take notes (digital or
printed) from the discussions.  

[] I, A0093910H, did not follow the class rules regarding homework
assignment, because of the following reason:

I suggest that I should be graded as follows:

== References ==

1. NLTK XMLCorpusReader
http://www.nltk.org/_modules/nltk/corpus/reader/xmldocs.html#XMLCorpusReader.xml

2. Parsing XML and The ElementTree XML API
https://docs.python.org/2/library/xml.etree.elementtree.html

3. How to get indices of a sorted array in python
# http://stackoverflow.com/questions/6422700/how-to-get-indices-of-a-sorted-array-in-python

4. NLTK stopwords
http://www.nltk.org/book/ch02.html