{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Homework #1 - The Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This skeleton file is provided for HW#1 only.  You are expected to modify it for use in this and other homeworks for the course.)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the README section for A0093910H's submission.\n",
    "(For group submissions [when applicable], simply concatenate the student matric numbers in lexicographical order separated by a '-' (dash); e.g., A0000000X-A0000001Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Notes about this assignment \n",
    "\n",
    "The code below executes in the following order:\n",
    "\n",
    "- load training data using pandas library\n",
    "- Get the training data as numpy arrays\n",
    "- Add bias to the x data, increasing its dimension from 20 to 21\n",
    "- Perform logistic regression on training data, using batch/single-point selection and different values for η (0.05 and 0.005)\n",
    "- Evaulate the resulting weights using out-of-sample-error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files included with this submission\n",
    "\n",
    "hw1-1.ipynb\n",
    "- source code and answer to the essay question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import matplotlib.pyplot as pl\n",
    "%matplotlib inline\n",
    "# Plotting with style! \n",
    "import seaborn as sb \n",
    "\n",
    "# Size the plot appropriately for online display\n",
    "pl.rcParams['figure.figsize'] = (12.0, 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix the random number generator first, in case we need results that are replicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nr.seed(3244)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets\n",
    "We also add a bias column of 1s for training data x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def add_bias_column(x):\n",
    "    return np.insert(x, 0, 1, axis=1)\n",
    "\n",
    "input_dimension = 20\n",
    "weight_dimension = input_dimension + 1 # add bias\n",
    "training_data = pd.read_table('hw1-train.dat', delim_whitespace=True, header=None)\n",
    "x_input = training_data.values[:,:input_dimension]\n",
    "y_input = training_data.values[:,input_dimension]\n",
    "N = len(x_input)\n",
    "x_input = add_bias_column(x_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR\n",
    "\n",
    "`learn_batch` function is used for batch selection.\n",
    "\n",
    "`learn_single_point` function is used for deterministic single point selection.\n",
    "\n",
    "Both use `lr` function as the underlying learning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch learning with 2333 iterations and eta of 0.05\n",
      "batch learning with 2333 iterations and eta of 0.005\n",
      "single point learning with 2333 iterations and eta of 0.05\n",
      "single point learning with 2333 iterations and eta of 0.005\n"
     ]
    }
   ],
   "source": [
    "def compute_single_point_gradient(xi, yi, current_w):\n",
    "    numerator = yi * xi\n",
    "    denominator = 1 + math.exp(yi * current_w.dot(xi))\n",
    "    return (numerator / denominator)\n",
    "\n",
    "def compute_ave_gradient(xn, yn, current_w):\n",
    "    gradient_sum = np.zeros(weight_dimension)\n",
    "    for idx in range(N):\n",
    "        gradient_sum += compute_single_point_gradient(xn[idx], yn[idx], current_w)\n",
    "    ave_gradient = (-1 / N) * gradient_sum\n",
    "    return ave_gradient\n",
    "\n",
    "def compute_new_weight(ave_gradient, current_w, eta):\n",
    "    return current_w + eta * (ave_gradient * (-1))\n",
    "\n",
    "def lr(current_w, eta, use_single_point = False, iteration_number = 0):\n",
    "    if use_single_point:\n",
    "        idx = iteration_number % N\n",
    "        gradient = -compute_single_point_gradient(x_input[idx], y_input[idx], current_w)\n",
    "    else:\n",
    "        gradient = compute_ave_gradient(x_input, y_input, current_w)\n",
    "    return compute_new_weight(gradient, current_w, eta)\n",
    "\n",
    "def compute_single_in_sample_error(xi, yi, current_w):\n",
    "    return np.log(1 + math.exp((-yi) * current_w.dot(xi)))\n",
    "\n",
    "def compute_ave_in_sample_error(xn, yn, current_w):\n",
    "    err_sum = 0\n",
    "    for idx in range(N):\n",
    "        err_sum += compute_single_in_sample_error(xn[idx], yn[idx], current_w)\n",
    "    err_ave = (1 / N) * err_sum\n",
    "    return err_ave\n",
    "\n",
    "def learn_batch(iterations, eta):\n",
    "    print('batch learning with ' + str(iterations) + ' iterations and eta of ' + str(eta))\n",
    "    w = np.zeros(weight_dimension)\n",
    "    for x in range(iterations):\n",
    "        w = lr(w, eta, False)\n",
    "    return w\n",
    "\n",
    "def learn_single_point(iterations, eta):\n",
    "    print('single point learning with ' + str(iterations) + ' iterations and eta of ' + str(eta))\n",
    "    w = np.zeros(weight_dimension)\n",
    "    for x in range(iterations):\n",
    "        w = lr(w, eta, True, x)\n",
    "    return w\n",
    "\n",
    "w1 = learn_batch(2333, 0.05)\n",
    "w2 = learn_batch(2333, 0.005)\n",
    "w3 = learn_single_point(2333, 0.05)\n",
    "w4 = learn_single_point(2333, 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testing_data = pd.read_table('hw1-test.dat', delim_whitespace=True, header=None)\n",
    "x_test = testing_data.values[:,:input_dimension]\n",
    "x_test = add_bias_column(x_test)\n",
    "y_test = testing_data.values[:,input_dimension]\n",
    "N_test = len(x_test)\n",
    "\n",
    "def check_weight_against_test(weight, test_xi, test_yi):\n",
    "    dot_product = weight.dot(test_xi)\n",
    "    # print(dot_product)\n",
    "    h_x = math.exp(dot_product) / (1 + math.exp(dot_product))\n",
    "    prediction = 1 if (h_x >= 0.5) else -1\n",
    "    # print(prediction == test_yi)\n",
    "    return (prediction == test_yi)\n",
    "\n",
    "def compute_out_of_sample_error(weight):\n",
    "    err_sum = 0\n",
    "    for idx in range(N_test):\n",
    "        if(check_weight_against_test(weight, x_test[idx], y_test[idx]) != True):\n",
    "            err_sum += 1\n",
    "    err_ave = (1 / N_test) * err_sum\n",
    "    return err_ave\n",
    "\n",
    "def print_result(weight):\n",
    "    print('weight:')\n",
    "    print(weight.tolist())\n",
    "    # print('in-sample error')\n",
    "    # print(compute_ave_in_sample_error(x_input, y_input, weight))\n",
    "    print('out-sample error:')\n",
    "    print(compute_out_of_sample_error(weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result for η = 0.05 for T = 2333 (batch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.11619989953257741,\n",
       " -0.6230638861458145,\n",
       " 0.8305469786461498,\n",
       " -1.0934973403011155,\n",
       " 0.05572273780420706,\n",
       " -1.1139138777344821,\n",
       " -0.01296554708163877,\n",
       " 1.1124953425644475,\n",
       " -0.8158812303486359,\n",
       " 0.4309260722643113,\n",
       " 1.4234615491768567,\n",
       " 0.2768854305700301,\n",
       " -0.8809569714738535,\n",
       " -0.5974162096293618,\n",
       " 0.8570422509145481,\n",
       " 1.1536100733824346,\n",
       " 1.3039896671074884,\n",
       " -1.3480710066622437,\n",
       " 1.3424348786513611,\n",
       " -0.6163682044354374,\n",
       " -1.1006430680003119]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18433333333333332"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_out_of_sample_error(w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result for η = 0.005 for T = 2333 (batch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0068855503106714875,\n",
       " -0.11440970104349153,\n",
       " 0.1713339205118567,\n",
       " -0.2195381778091197,\n",
       " 0.031431981110603205,\n",
       " -0.23767226697862284,\n",
       " 0.01827846966222285,\n",
       " 0.21209126258519115,\n",
       " -0.16241841680388647,\n",
       " 0.08772493345745595,\n",
       " 0.31631168396518783,\n",
       " 0.05802731776823319,\n",
       " -0.15479095606960339,\n",
       " -0.09603835038685485,\n",
       " 0.19544843544259954,\n",
       " 0.2583708726720174,\n",
       " 0.27707535469407557,\n",
       " -0.29100915213236694,\n",
       " 0.2758915978261983,\n",
       " -0.12857103153081947,\n",
       " -0.2300694440172345]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26366666666666666"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_out_of_sample_error(w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result for η = 0.05 for T = 2333 (single point):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.13610500359911648,\n",
       " -0.7150913932415003,\n",
       " 0.8717352150115324,\n",
       " -1.1547142455013584,\n",
       " -0.045438363710803155,\n",
       " -1.115596647678838,\n",
       " -0.05679034454126214,\n",
       " 1.083705786851147,\n",
       " -0.9282516677434588,\n",
       " 0.4247163020703131,\n",
       " 1.4044058735614784,\n",
       " 0.178292970315381,\n",
       " -0.7907827543185035,\n",
       " -0.6985461402096789,\n",
       " 0.8544660529869273,\n",
       " 1.1306861114378426,\n",
       " 1.2962022062852565,\n",
       " -1.4799120604127372,\n",
       " 1.4143478871384088,\n",
       " -0.6600702563762323,\n",
       " -1.0755395024117327]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w3.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22266666666666665"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_out_of_sample_error(w3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result for η = 0.005 for T = 2333 (single point):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.00587569360276918,\n",
       " -0.13261359159470548,\n",
       " 0.17183769594011664,\n",
       " -0.23245823928057666,\n",
       " -0.0012891321734608944,\n",
       " -0.247474908095806,\n",
       " 0.004545199830573823,\n",
       " 0.20677978266374542,\n",
       " -0.1799303285184058,\n",
       " 0.08439129567952228,\n",
       " 0.30812906316812844,\n",
       " 0.034969577252142864,\n",
       " -0.14550272003141143,\n",
       " -0.10943277154337429,\n",
       " 0.18726770702824713,\n",
       " 0.24725378855802663,\n",
       " 0.2687787882441544,\n",
       " -0.3136832011330746,\n",
       " 0.27764022295336366,\n",
       " -0.1388937393317604,\n",
       " -0.22247117336152533]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w4.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19333333333333333"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_out_of_sample_error(w4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essay Questions\n",
    "\n",
    "_You may choose to do the essay questions here in the .ipynb notebook, but you are welcomed to use a word processor instead and write your solutions there instead (and convert it into .pdf format).  If you do that, please ensure to delete this section._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. [LFD Exercise 1.2] Suppose that we use a perceptron to detect spam messages. Let's say that each email messages represented by the frequency of occurrence of keywords, and the output is +1 if the message is considered spam.\n",
    "\n",
    "    1. Can you think of some keywords that will end up with a large positive weight into perceptron?\n",
    "\n",
    "    2. How about keywords that will get a negative weight?\n",
    "\n",
    "    3. What parameter in the perceptron directly affects how many borderline messages end up classified as spam?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "- \"ads\"\n",
    "- \"lottery\"\n",
    "- \"win\"\n",
    "\n",
    "2.\n",
    "\n",
    "- \"account\"\n",
    "- \"password\"\n",
    "- \"booking\"\n",
    "- \"reservation\"\n",
    "- \"important\"\n",
    "\n",
    "3.\n",
    "\n",
    "the bias value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Consider a coin tossing experiment. You toss a coin 100 times, with the result of heads 70 times and tails 30 times. We denote the probability of heads of this coin as Θ. Now consider a coin toss.\n",
    "\n",
    "    1. Build a model using maximum lilkelihood estimation (MLE) to infer Θ.\n",
    "\n",
    "    2. Can we judge that this is an unfair coin? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "\n",
    "Assume prior belief $= P(Θ) = 0.5$\n",
    "\n",
    "By Bayes Theorem, probability density function of Θ given observations:\n",
    "\n",
    "\\begin{aligned}\n",
    "p(Θ\\vert H=70,T=30)\n",
    "&={\\frac {P(H=30\\,\\vert\\,Θ,N=100)\\,p(Θ)}{p(H=70,T=30)}} \\\\\n",
    "&={\\frac {{100 \\choose 30}\\,Θ^{70}\\,(1-Θ)^{30}\\times 0.5}{1}} \\\\\n",
    "&=0.5{100 \\choose 30}\\,Θ^{70}\\,(1-Θ)^{30}\n",
    "\\end{aligned}\n",
    "\n",
    "Maximize $p(Θ\\vert H=70,T=30)$ by differentiation,\n",
    "\n",
    "\\begin{aligned}\n",
    "{\\frac {dp(Θ\\vert H=70,T=30)}{dΘ}} = 0.5{100 \\choose 30}\\,Θ^{69}\\,(1-Θ)^{29}(100Θ-70) = 0 \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "\\begin{aligned}\n",
    "Θ = 0.7\n",
    "\\end{aligned}\n",
    "\n",
    "2.\n",
    "\n",
    "True probablity of head of a fair coin = 0.5\n",
    "\n",
    "At 95% confidence level,\n",
    "\n",
    "standard error:\n",
    "\\begin{aligned}\n",
    "s = {\\sqrt {\\frac {p\\,(1-p)}{n}}}\\leq {\\sqrt {\\frac {0.5\\times 0.5}{100}}}={\\frac {1}{20}}\n",
    "\\end{aligned}\n",
    "Maxium error:\n",
    "\\begin{aligned}\n",
    "E=Zs={\\frac {Z}{20}}={\\frac {1.96}{20}}=0.098\n",
    "\\end{aligned}\n",
    "Hence, for a fair coin, the probablity of having a head $Θ'$ should satisfy: \n",
    "\n",
    "\\begin{aligned}\n",
    "0.5-0.098<\\;&Θ'<0.5+0.098 \\\\\n",
    "0.402<\\;&Θ'<0.598\n",
    "\\end{aligned}\n",
    "\n",
    "0.7 clearly does not fall into the interval, hence we conclude that the coin is not fair at 95% confidence level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. In the programming logistic regression, part (c), we did away with the stochastic idea of SGD and substituted a round-robin version, which deterministically uses the next point in turn to perform the gradient descent. Describe whether you think this is a good robust idea or not for datasets in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might not be a good robust idea for datasets in general.\n",
    "\n",
    "If the dataset is random in nature, then the round-robin version would effectively be the same as SGD. However, realworld datasets may have certain intrinsic patterns within the data. This coupled with the round-robin would tilt the training result depending on the number of iterations and the size of the data. \n",
    "\n",
    "For example, imagine the case where the training data has distinctive patterns for the first half and the second half of data points. The training result obtained when round-robin stopped at the middle of the data might be drastically different from the result obtained when round-robin stopped at the end of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statement of Individual Work\n",
    "\n",
    "Please initial (between the square brackets) one of the following statements.\n",
    "\n",
    "[X] I, A0093910H, certify that I have followed the CS 3244 Machine Learning class guidelines for homework assignments.  In particular, I expressly vow that I have followed the Facebook rule in discussing with others in doing the assignment and did not take notes (digital or printed) from the discussions.  \n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "I have refered to the following list of people and websites in preparing my homework submission:\n",
    "\n",
    "- Zhou Yichen\n",
    "- [Pandas](http://pandas.pydata.org/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
